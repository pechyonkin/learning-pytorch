{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "DATA_PATH = Path(\"../data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"http://deeplearning.net/data/mnist/\"\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "    content = requests.get(URL + FILENAME).content\n",
    "    (PATH / FILENAME).open(\"wb\").write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "\n",
    "pyplot.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\")\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train, x_valid, y_valid) = map(torch.tensor, (x_train, y_train, x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, c = x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\n",
      "torch.Size([50000, 784])\n",
      "tensor(0) tensor(9)\n"
     ]
    }
   ],
   "source": [
    "print(x_train, y_train)\n",
    "print(x_train.shape)\n",
    "print(y_train.min(), y_train.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0441, -0.0079, -0.0361,  ...,  0.0252, -0.0017,  0.0179],\n",
       "        [-0.0163, -0.0772,  0.0680,  ..., -0.0309, -0.0289,  0.0004],\n",
       "        [ 0.0211,  0.0252, -0.0423,  ..., -0.0485,  0.0521, -0.0077],\n",
       "        ...,\n",
       "        [ 0.0105, -0.0759,  0.0074,  ...,  0.0138, -0.0313,  0.0296],\n",
       "        [ 0.0202,  0.0561, -0.0228,  ..., -0.0072,  0.0103,  0.0480],\n",
       "        [-0.0002,  0.0476,  0.0005,  ..., -0.1027,  0.0195,  0.0265]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    # x is (batch, classes) shape\n",
    "    # x.exp().sum(-1).log() is (batch) shape, so need to unsqueeze for broadcasting\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "def model(xb):\n",
    "    return log_softmax(xb @ weights + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "\n",
    "xb = x_train[0:bs]\n",
    "preds = model(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.0784, -2.4136, -2.1865, -2.4199, -2.9380, -2.2734, -2.0478, -2.8368,\n",
      "        -1.9763, -2.2830], grad_fn=<SelectBackward>)\n",
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "print(preds[0])\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(input, target):\n",
    "    ''' Negative log likelihood, note that log was already applied to input'''\n",
    "    # input: (B, 10) - log softmax\n",
    "    # target (B)\n",
    "    # returns average negative log likelihood for the batch\n",
    "    return -input[range(target.shape[0]), target].mean()\n",
    "\n",
    "loss_func = nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3397, grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "yb = y_train[0:bs]\n",
    "print(loss_func(preds, yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, yb):\n",
    "    # out: (B, 10)\n",
    "    # yb: (B)\n",
    "    preds = out.argmax(-1) # (B)\n",
    "    return (preds == yb).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0625)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3397, grad_fn=<NegBackward>)\n",
      "tensor(0.9230, grad_fn=<NegBackward>)\n",
      "tensor(0.6584, grad_fn=<NegBackward>)\n",
      "tensor(0.4684, grad_fn=<NegBackward>)\n",
      "tensor(0.4317, grad_fn=<NegBackward>)\n",
      "tensor(0.4115, grad_fn=<NegBackward>)\n",
      "tensor(0.3105, grad_fn=<NegBackward>)\n",
      "tensor(0.3729, grad_fn=<NegBackward>)\n",
      "tensor(0.7740, grad_fn=<NegBackward>)\n",
      "tensor(0.2922, grad_fn=<NegBackward>)\n",
      "tensor(0.3155, grad_fn=<NegBackward>)\n",
      "tensor(0.2968, grad_fn=<NegBackward>)\n",
      "tensor(0.3563, grad_fn=<NegBackward>)\n",
      "tensor(0.2627, grad_fn=<NegBackward>)\n",
      "tensor(0.3172, grad_fn=<NegBackward>)\n",
      "tensor(0.2730, grad_fn=<NegBackward>)\n",
      "tensor(0.3652, grad_fn=<NegBackward>)\n",
      "tensor(0.2839, grad_fn=<NegBackward>)\n",
      "tensor(0.6398, grad_fn=<NegBackward>)\n",
      "tensor(0.4583, grad_fn=<NegBackward>)\n",
      "tensor(0.2984, grad_fn=<NegBackward>)\n",
      "tensor(0.2352, grad_fn=<NegBackward>)\n",
      "tensor(0.4548, grad_fn=<NegBackward>)\n",
      "tensor(0.5952, grad_fn=<NegBackward>)\n",
      "tensor(0.2707, grad_fn=<NegBackward>)\n",
      "tensor(0.5572, grad_fn=<NegBackward>)\n",
      "tensor(0.4540, grad_fn=<NegBackward>)\n",
      "tensor(0.2187, grad_fn=<NegBackward>)\n",
      "tensor(0.2632, grad_fn=<NegBackward>)\n",
      "tensor(0.4582, grad_fn=<NegBackward>)\n",
      "tensor(0.3842, grad_fn=<NegBackward>)\n",
      "tensor(0.2398, grad_fn=<NegBackward>)\n",
      "tensor(0.2264, grad_fn=<NegBackward>)\n",
      "tensor(0.3209, grad_fn=<NegBackward>)\n",
      "tensor(0.0989, grad_fn=<NegBackward>)\n",
      "tensor(0.3110, grad_fn=<NegBackward>)\n",
      "tensor(0.4533, grad_fn=<NegBackward>)\n",
      "tensor(0.5951, grad_fn=<NegBackward>)\n",
      "tensor(0.2486, grad_fn=<NegBackward>)\n",
      "tensor(0.3544, grad_fn=<NegBackward>)\n",
      "tensor(0.2394, grad_fn=<NegBackward>)\n",
      "tensor(0.3548, grad_fn=<NegBackward>)\n",
      "tensor(0.5106, grad_fn=<NegBackward>)\n",
      "tensor(0.2217, grad_fn=<NegBackward>)\n",
      "tensor(0.3127, grad_fn=<NegBackward>)\n",
      "tensor(0.2135, grad_fn=<NegBackward>)\n",
      "tensor(0.2367, grad_fn=<NegBackward>)\n",
      "tensor(0.4094, grad_fn=<NegBackward>)\n",
      "tensor(0.3725, grad_fn=<NegBackward>)\n",
      "tensor(0.5878, grad_fn=<NegBackward>)\n",
      "tensor(0.3859, grad_fn=<NegBackward>)\n",
      "tensor(0.4431, grad_fn=<NegBackward>)\n",
      "tensor(0.3878, grad_fn=<NegBackward>)\n",
      "tensor(0.1679, grad_fn=<NegBackward>)\n",
      "tensor(0.3321, grad_fn=<NegBackward>)\n",
      "tensor(0.3632, grad_fn=<NegBackward>)\n",
      "tensor(0.2802, grad_fn=<NegBackward>)\n",
      "tensor(0.1946, grad_fn=<NegBackward>)\n",
      "tensor(0.4309, grad_fn=<NegBackward>)\n",
      "tensor(0.2947, grad_fn=<NegBackward>)\n",
      "tensor(0.2577, grad_fn=<NegBackward>)\n",
      "tensor(0.1645, grad_fn=<NegBackward>)\n",
      "tensor(0.2862, grad_fn=<NegBackward>)\n",
      "tensor(0.2836, grad_fn=<NegBackward>)\n",
      "tensor(0.4846, grad_fn=<NegBackward>)\n",
      "tensor(0.2259, grad_fn=<NegBackward>)\n",
      "tensor(0.1652, grad_fn=<NegBackward>)\n",
      "tensor(0.4784, grad_fn=<NegBackward>)\n",
      "tensor(0.3570, grad_fn=<NegBackward>)\n",
      "tensor(0.2110, grad_fn=<NegBackward>)\n",
      "tensor(0.3803, grad_fn=<NegBackward>)\n",
      "tensor(0.6402, grad_fn=<NegBackward>)\n",
      "tensor(0.4740, grad_fn=<NegBackward>)\n",
      "tensor(0.4272, grad_fn=<NegBackward>)\n",
      "tensor(0.3188, grad_fn=<NegBackward>)\n",
      "tensor(0.2180, grad_fn=<NegBackward>)\n",
      "tensor(0.3071, grad_fn=<NegBackward>)\n",
      "tensor(0.2125, grad_fn=<NegBackward>)\n",
      "tensor(0.2361, grad_fn=<NegBackward>)\n",
      "tensor(0.2846, grad_fn=<NegBackward>)\n",
      "tensor(0.3514, grad_fn=<NegBackward>)\n",
      "tensor(0.3731, grad_fn=<NegBackward>)\n",
      "tensor(0.2073, grad_fn=<NegBackward>)\n",
      "tensor(0.2034, grad_fn=<NegBackward>)\n",
      "tensor(0.3313, grad_fn=<NegBackward>)\n",
      "tensor(0.1338, grad_fn=<NegBackward>)\n",
      "tensor(0.2629, grad_fn=<NegBackward>)\n",
      "tensor(0.6280, grad_fn=<NegBackward>)\n",
      "tensor(0.2135, grad_fn=<NegBackward>)\n",
      "tensor(0.2604, grad_fn=<NegBackward>)\n",
      "tensor(0.2136, grad_fn=<NegBackward>)\n",
      "tensor(0.2515, grad_fn=<NegBackward>)\n",
      "tensor(0.1652, grad_fn=<NegBackward>)\n",
      "tensor(0.2391, grad_fn=<NegBackward>)\n",
      "tensor(0.1994, grad_fn=<NegBackward>)\n",
      "tensor(0.2317, grad_fn=<NegBackward>)\n",
      "tensor(0.2112, grad_fn=<NegBackward>)\n",
      "tensor(0.5622, grad_fn=<NegBackward>)\n",
      "tensor(0.4662, grad_fn=<NegBackward>)\n",
      "tensor(0.1966, grad_fn=<NegBackward>)\n",
      "tensor(0.1941, grad_fn=<NegBackward>)\n",
      "tensor(0.3817, grad_fn=<NegBackward>)\n",
      "tensor(0.5813, grad_fn=<NegBackward>)\n",
      "tensor(0.1812, grad_fn=<NegBackward>)\n",
      "tensor(0.4904, grad_fn=<NegBackward>)\n",
      "tensor(0.3673, grad_fn=<NegBackward>)\n",
      "tensor(0.1489, grad_fn=<NegBackward>)\n",
      "tensor(0.2325, grad_fn=<NegBackward>)\n",
      "tensor(0.4138, grad_fn=<NegBackward>)\n",
      "tensor(0.3418, grad_fn=<NegBackward>)\n",
      "tensor(0.1664, grad_fn=<NegBackward>)\n",
      "tensor(0.1877, grad_fn=<NegBackward>)\n",
      "tensor(0.2909, grad_fn=<NegBackward>)\n",
      "tensor(0.0863, grad_fn=<NegBackward>)\n",
      "tensor(0.2602, grad_fn=<NegBackward>)\n",
      "tensor(0.4355, grad_fn=<NegBackward>)\n",
      "tensor(0.5400, grad_fn=<NegBackward>)\n",
      "tensor(0.2039, grad_fn=<NegBackward>)\n",
      "tensor(0.2911, grad_fn=<NegBackward>)\n",
      "tensor(0.2131, grad_fn=<NegBackward>)\n",
      "tensor(0.3105, grad_fn=<NegBackward>)\n",
      "tensor(0.4911, grad_fn=<NegBackward>)\n",
      "tensor(0.2212, grad_fn=<NegBackward>)\n",
      "tensor(0.2528, grad_fn=<NegBackward>)\n",
      "tensor(0.1767, grad_fn=<NegBackward>)\n",
      "tensor(0.1895, grad_fn=<NegBackward>)\n",
      "tensor(0.3662, grad_fn=<NegBackward>)\n",
      "tensor(0.3467, grad_fn=<NegBackward>)\n",
      "tensor(0.5363, grad_fn=<NegBackward>)\n",
      "tensor(0.3580, grad_fn=<NegBackward>)\n",
      "tensor(0.4523, grad_fn=<NegBackward>)\n",
      "tensor(0.4025, grad_fn=<NegBackward>)\n",
      "tensor(0.1491, grad_fn=<NegBackward>)\n",
      "tensor(0.2952, grad_fn=<NegBackward>)\n",
      "tensor(0.3239, grad_fn=<NegBackward>)\n",
      "tensor(0.2453, grad_fn=<NegBackward>)\n",
      "tensor(0.1727, grad_fn=<NegBackward>)\n",
      "tensor(0.4377, grad_fn=<NegBackward>)\n",
      "tensor(0.2679, grad_fn=<NegBackward>)\n",
      "tensor(0.2247, grad_fn=<NegBackward>)\n",
      "tensor(0.1508, grad_fn=<NegBackward>)\n",
      "tensor(0.2520, grad_fn=<NegBackward>)\n",
      "tensor(0.2808, grad_fn=<NegBackward>)\n",
      "tensor(0.4809, grad_fn=<NegBackward>)\n",
      "tensor(0.1961, grad_fn=<NegBackward>)\n",
      "tensor(0.1444, grad_fn=<NegBackward>)\n",
      "tensor(0.4305, grad_fn=<NegBackward>)\n",
      "tensor(0.3527, grad_fn=<NegBackward>)\n",
      "tensor(0.1726, grad_fn=<NegBackward>)\n",
      "tensor(0.3642, grad_fn=<NegBackward>)\n",
      "tensor(0.6062, grad_fn=<NegBackward>)\n",
      "tensor(0.4440, grad_fn=<NegBackward>)\n",
      "tensor(0.4377, grad_fn=<NegBackward>)\n",
      "tensor(0.2789, grad_fn=<NegBackward>)\n",
      "tensor(0.1922, grad_fn=<NegBackward>)\n",
      "tensor(0.2922, grad_fn=<NegBackward>)\n",
      "tensor(0.2041, grad_fn=<NegBackward>)\n",
      "tensor(0.2081, grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "lr = 0.5\n",
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # (n - 1) because starts at 0\n",
    "    # + 1 because\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i: end_i]\n",
    "        yb = y_train[start_i: end_i]\n",
    "        \n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "        \n",
    "        if (i % 10) == 0:\n",
    "            print(loss)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0809, grad_fn=<NegBackward>) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `torch.nn.functional`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
